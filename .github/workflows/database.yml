name: Database Operations

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'migrations/**'
      - 'apps/api/src/db/**'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'migrations/**'
      - 'apps/api/src/db/**'
  workflow_dispatch:
    inputs:
      operation:
        description: 'Database operation to perform'
        required: true
        default: 'migrate'
        type: choice
        options:
        - migrate
        - seed
        - backup
        - reset-test
        - analyze
      environment:
        description: 'Target environment'
        required: true
        default: 'staging'
        type: choice
        options:
        - staging
        - production
        - test

env:
  NODE_VERSION: '18'
  PNPM_VERSION: '8'

jobs:
  validate-migrations:
    name: Validate Database Migrations
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'pnpm'

    - name: Install pnpm
      uses: pnpm/action-setup@v2
      with:
        version: ${{ env.PNPM_VERSION }}

    - name: Get pnpm store directory
      shell: bash
      run: |
        echo "STORE_PATH=$(pnpm store path --silent)" >> $GITHUB_ENV

    - name: Setup pnpm cache
      uses: actions/cache@v3
      with:
        path: ${{ env.STORE_PATH }}
        key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
        restore-keys: |
          ${{ runner.os }}-pnpm-store-

    - name: Install dependencies
      run: pnpm install --frozen-lockfile

    - name: Validate migration files
      run: |
        echo "Validating database migration files..."

        # Check if migrations directory exists and has files
        if [ ! -d "migrations" ]; then
          echo "âŒ Migrations directory not found"
          exit 1
        fi

        MIGRATION_COUNT=$(find migrations -name "*.sql" | wc -l)
        echo "Found $MIGRATION_COUNT migration files"

        if [ $MIGRATION_COUNT -eq 0 ]; then
          echo "âš ï¸ No migration files found"
        fi

        # Validate migration file naming convention
        for migration in migrations/*.sql; do
          if [ -f "$migration" ]; then
            filename=$(basename "$migration")
            if [[ ! "$filename" =~ ^[0-9]{3}_.+\.sql$ ]]; then
              echo "âŒ Invalid migration filename format: $filename (expected: XXX_name.sql)"
              exit 1
            fi

            # Check if migration file has proper content
            if [ ! -s "$migration" ]; then
              echo "âŒ Migration file is empty: $filename"
              exit 1
            fi

            echo "âœ… Valid migration file: $filename"
          fi
        done

        # Check for duplicate migration numbers
        MIGRATION_NUMBERS=$(find migrations -name "*.sql" -exec basename {} \; | cut -d'_' -f1 | sort | uniq -d)
        if [ -n "$MIGRATION_NUMBERS" ]; then
          echo "âŒ Duplicate migration numbers found: $MIGRATION_NUMBERS"
          exit 1
        fi

        echo "âœ… All migration files validated successfully"

    - name: Validate database schema
      run: |
        echo "Validating database schema consistency..."

        # This would typically validate your schema files
        # Example: Check if schema definitions are consistent

        echo "âœ… Database schema validation completed"

  test-migrations:
    name: Test Database Migrations
    runs-on: ubuntu-latest
    needs: validate-migrations

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: parsify_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'pnpm'

    - name: Install pnpm
      uses: pnpm/action-setup@v2
      with:
        version: ${{ env.PNPM_VERSION }}

    - name: Get pnpm store directory
      shell: bash
      run: |
        echo "STORE_PATH=$(pnpm store path --silent)" >> $GITHUB_ENV

    - name: Setup pnpm cache
      uses: actions/cache@v3
      with:
        path: ${{ env.STORE_PATH }}
        key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
        restore-keys: |
          ${{ runner.os }}-pnpm-store-

    - name: Install dependencies
      run: pnpm install --frozen-lockfile

    - name: Wait for PostgreSQL
      run: |
        echo "Waiting for PostgreSQL to be ready..."
        timeout 60 bash -c 'until pg_isready -h localhost -p 5432; do sleep 2; done'

    - name: Run migrations on test database
      run: |
        echo "Running migrations on test database..."

        # Set up database connection
        export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/parsify_test"

        # Run migrations (adjust command based on your setup)
        # For Wrangler D1, this might be different
        # pnpm db:migrate --env test

        # For PostgreSQL example:
        for migration in migrations/*.sql; do
          if [ -f "$migration" ]; then
            echo "Running migration: $(basename "$migration")"
            psql "$DATABASE_URL" -f "$migration"
          fi
        done

        echo "âœ… Migrations completed successfully"

    - name: Validate migrated database
      run: |
        echo "Validating migrated database structure..."

        export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/parsify_test"

        # Check if expected tables exist
        # psql "$DATABASE_URL" -c "\dt"

        # Run database schema validation
        # Add your schema validation logic here

        echo "âœ… Database validation completed"

    - name: Seed test data
      run: |
        echo "Seeding test data..."

        export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/parsify_test"

        # Run seed script if exists
        if [ -f "migrations/seed.sql" ]; then
          psql "$DATABASE_URL" -f migrations/seed.sql
        fi

        echo "âœ… Test data seeded"

    - name: Run database tests
      run: |
        echo "Running database-related tests..."

        # Run tests that interact with the database
        # pnpm test:db --env test

        echo "âœ… Database tests completed"

  database-backup:
    name: Database Backup
    runs-on: ubuntu-latest
    if: github.event.inputs.operation == 'backup' || github.event_name == 'schedule'
    environment: ${{ github.event.inputs.environment || 'staging' }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}

    - name: Create database backup
      run: |
        echo "Creating database backup for ${{ github.event.inputs.environment || 'staging' }} environment..."

        BACKUP_FILE="backup-${{ github.event.inputs.environment || 'staging' }}-$(date +%Y%m%d-%H%M%S).sql"

        # Create backup (adjust based on your database)
        # For PostgreSQL:
        # pg_dump "$DATABASE_URL" > "$BACKUP_FILE"

        # For Wrangler D1:
        # wrangler d1 export parsify-dev --output "$BACKUP_FILE"

        echo "âœ… Database backup created: $BACKUP_FILE"

    - name: Upload backup to storage
      run: |
        echo "Uploading backup to storage..."

        # Upload to your preferred storage solution
        # AWS S3 example:
        # aws s3 cp "$BACKUP_FILE" "s3://your-backup-bucket/database/"

        echo "âœ… Backup uploaded to storage"

    - name: Clean up old backups
      run: |
        echo "Cleaning up old backups (keeping last 30 days)..."

        # Clean up old backups from storage
        # aws s3 ls s3://your-backup-bucket/database/ | while read -r line; do
        #   createDate=$(echo "$line" | awk '{print $1" "$2}')
        #   createDate=$(date -d "$createDate" +%s)
        #   olderThan=$(date -d "30 days ago" +%s)
        #   if [[ $createDate -lt $olderThan ]]; then
        #     fileName=$(echo "$line" | awk '{print $4}')
        #     aws s3 rm "s3://your-backup-bucket/database/$fileName"
        #   fi
        # done

        echo "âœ… Old backups cleaned up"

  deploy-migrations:
    name: Deploy Database Migrations
    runs-on: ubuntu-latest
    if: github.event.inputs.operation == 'migrate' || github.ref == 'refs/heads/main'
    environment: ${{ github.event.inputs.environment || 'staging' }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'pnpm'

    - name: Install pnpm
      uses: pnpm/action-setup@v2
      with:
        version: ${{ env.PNPM_VERSION }}

    - name: Install dependencies
      run: pnpm install --frozen-lockfile

    - name: Create pre-migration backup
      run: |
        echo "Creating pre-migration backup..."

        BACKUP_FILE="pre-migration-backup-$(date +%Y%m%d-%H%M%S).sql"

        # Create backup before running migrations
        # Add your backup command here

        echo "âœ… Pre-migration backup created: $BACKUP_FILE"

    - name: Deploy migrations
      run: |
        echo "Deploying migrations to ${{ github.event.inputs.environment || 'staging' }} environment..."

        # Run migrations on target environment
        # For Wrangler D1:
        # pnpm db:migrate --env ${{ github.event.inputs.environment || 'staging' }}

        # For other databases:
        # Add your migration deployment command here

        echo "âœ… Migrations deployed successfully"

    - name: Verify migration success
      run: |
        echo "Verifying migration success..."

        # Add verification steps
        # Check database schema, run health checks, etc.

        echo "âœ… Migration verification completed"

    - name: Update migration status
      uses: actions/github-script@v6
      with:
        script: |
          await github.rest.repos.createCommitStatus({
            owner: context.repo.owner,
            repo: context.repo.repo,
            sha: context.sha,
            state: 'success',
            target_url: 'https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}',
            description: 'Database migrations completed successfully',
            context: 'database/migrations'
          });

  database-analytics:
    name: Database Analytics
    runs-on: ubuntu-latest
    if: github.event.inputs.operation == 'analyze' || github.event_name == 'schedule'
    environment: ${{ github.event.inputs.environment || 'staging' }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}

    - name: Run database analytics
      run: |
        echo "Running database analytics for ${{ github.event.inputs.environment || 'staging' }} environment..."

        mkdir -p analytics-results

        # Generate analytics report
        cat > analytics-results/database-analytics.md << 'EOF'
        # Database Analytics Report

        **Generated**: $(date -u)
        **Environment**: ${{ github.event.inputs.environment || 'staging' }}

        ## Database Statistics

        EOF

        # Add your analytics queries here
        # Example for PostgreSQL:
        # psql "$DATABASE_URL" -c "
        #   SELECT
        #     schemaname,
        #     tablename,
        #     n_tup_ins as inserts,
        #     n_tup_upd as updates,
        #     n_tup_del as deletes,
        #     n_live_tup as live_tuples,
        #     n_dead_tup as dead_tuples
        #   FROM pg_stat_user_tables;
        # " >> analytics-results/database-analytics.md

        # Database size analytics
        # Table size analytics
        # Query performance analytics
        # Index usage analytics

        echo "âœ… Database analytics completed"

    - name: Generate optimization recommendations
      run: |
        echo "Generating optimization recommendations..."

        cat >> analytics-results/database-analytics.md << 'EOF'

        ## Optimization Recommendations

        Based on the analytics data, consider the following optimizations:

        ### Index Optimization
        - Review unused indexes
        - Consider adding indexes for frequently queried columns

        ### Query Optimization
        - Identify slow queries for optimization
        - Review query execution plans

        ### Maintenance
        - Consider regular vacuum and analyze operations
        - Monitor table bloat

        EOF

        echo "âœ… Optimization recommendations generated"

    - name: Upload analytics results
      uses: actions/upload-artifact@v3
      with:
        name: database-analytics-results
        path: analytics-results/
        retention-days: 30

    - name: Create analytics issue (if needed)
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: 'ğŸ” Database Analytics Alert',
            body: `
            ## Database Analytics Alert

            **Environment**: ${{ github.event.inputs.environment || 'staging' }}
            **Time**: ${new Date().toISOString()}

            Database analytics have identified potential issues that require attention.

            Please review the detailed analytics report and take appropriate action.

            [View Analytics Report](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ context.runId }})
            `,
            labels: ['database', 'analytics', 'maintenance']
          });

  database-summary:
    name: Database Operations Summary
    runs-on: ubuntu-latest
    needs: [validate-migrations, test-migrations, database-backup, deploy-migrations, database-analytics]
    if: always()

    steps:
    - name: Generate operations summary
      run: |
        echo "## ğŸ—„ï¸ Database Operations Summary" > database-summary.md
        echo "" >> database-summary.md
        echo "**Generated**: $(date -u)" >> database-summary.md
        echo "**Repository**: ${{ github.repository }}" >> database-summary.md
        echo "**Commit**: ${{ github.sha }}" >> database-summary.md
        echo "" >> database-summary.md

        echo "### Operations Status" >> database-summary.md
        echo "" >> database-summary.md
        echo "| Operation | Status |" >> database-summary.md
        echo "|-----------|--------|" >> database-summary.md

        # Add status for each operation
        VALIDATE_STATUS="${{ needs.validate-migrations.result }}"
        TEST_STATUS="${{ needs.test-migrations.result }}"
        BACKUP_STATUS="${{ needs.database-backup.result }}"
        DEPLOY_STATUS="${{ needs.deploy-migrations.result }}"
        ANALYTICS_STATUS="${{ needs.database-analytics.result }}"

        echo "| Validate Migrations | $([ "$VALIDATE_STATUS" = "success" ] && echo "âœ… Success" || echo "âŒ Failed") |" >> database-summary.md
        echo "| Test Migrations | $([ "$TEST_STATUS" = "success" ] && echo "âœ… Success" || ([ "$TEST_STATUS" = "skipped" ] && echo "â­ï¸ Skipped" || echo "âŒ Failed")) |" >> database-summary.md
        echo "| Database Backup | $([ "$BACKUP_STATUS" = "success" ] && echo "âœ… Success" || ([ "$BACKUP_STATUS" = "skipped" ] && echo "â­ï¸ Skipped" || echo "âŒ Failed")) |" >> database-summary.md
        echo "| Deploy Migrations | $([ "$DEPLOY_STATUS" = "success" ] && echo "âœ… Success" || ([ "$DEPLOY_STATUS" = "skipped" ] && echo "â­ï¸ Skipped" || echo "âŒ Failed")) |" >> database-summary.md
        echo "| Database Analytics | $([ "$ANALYTICS_STATUS" = "success" ] && echo "âœ… Success" || ([ "$ANALYTICS_STATUS" = "skipped" ] && echo "â­ï¸ Skipped" || echo "âŒ Failed")) |" >> database-summary.md

        echo "" >> database-summary.md
        echo "### Migration Files" >> database-summary.md
        echo "" >> database-summary.md

        if [ -d "migrations" ]; then
          echo "Found $(find migrations -name "*.sql" | wc -l) migration files:" >> database-summary.md
          echo "" >> database-summary.md
          for migration in migrations/*.sql; do
            if [ -f "$migration" ]; then
              echo "- $(basename "$migration")" >> database-summary.md
            fi
          done
        else
          echo "No migrations directory found" >> database-summary.md
        fi

        echo "" >> database-summary.md
        echo "---" >> database-summary.md
        echo "*This summary covers all database operations performed in this workflow run*" >> database-summary.md

    - name: Upload database summary
      uses: actions/upload-artifact@v3
      with:
        name: database-operations-summary
        path: database-summary.md
        retention-days: 7

    - name: Comment PR with database results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');

          try {
            const summary = fs.readFileSync('database-summary.md', 'utf8');

            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ğŸ—„ï¸ Database Operations Results\n\n${summary}`
            });
          } catch (error) {
            console.log('Could not read database summary:', error.message);
          }
